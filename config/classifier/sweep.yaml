program: scripts/train_classifier.py
name: classifier_sweep
project: diffusion-gmm

method: bayes  # Bayesian optimization

metric:
  name: test_loss
  goal: minimize

parameters:

  classifier.batch_size:
    values: [64, 128, 256]

  classifier.lr:
    distribution: uniform
    max: 0.1
    min: 1e-2

  classifier.scheduler.CosineAnnealingWarmRestarts_kwargs.T_0:
    values: [200, 300, 600]

  classifier.scheduler.CosineAnnealingWarmRestarts_kwargs.eta_min:
    distribution: uniform
    max: 1e-2
    min: 1e-3

run_cap: 12

early_terminate:
  type: hyperband
  max_iter: 600
  eta: 3

command:
  - python
  - ${program}
  - ${args_no_hyphens}
  - wandb.log=true
  - classifier.train_data_dir=/stor/work/AMDG_Gilpin_Summer2024/vision_datasets/gmm_edm_imagenet64_all
  - classifier.n_props_train=1
  - classifier.n_runs=1
  - classifier.n_train_samples_per_class=1024
  - classifier.n_test_samples_per_class=1024
  - classifier.train_split=0.8
  - classifier.model.output_logit=false
  - classifier.model.use_bias=true
  - classifier.sweep_mode=true
  - classifier.save_dir=results/classifier_sweeps
  - classifier.save_name=classifier_sweep
  - classifier.verbose=true
  - rseed=99
  - run_name=10_class_softmax_bias_gmm


# Example usage:
# wandb sweep config/classifier/sweep.yaml